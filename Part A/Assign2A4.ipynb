{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11374628,"sourceType":"datasetVersion","datasetId":7121085}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"28d9be59","cell_type":"markdown","source":"# Imports","metadata":{}},{"id":"41d1020e","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport wandb\nimport os\nfrom PIL import Image\nimport random\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom torchsummary import summary\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Subset, DataLoader\nfrom sklearn.model_selection import StratifiedShuffleSplit\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:42:42.782143Z","iopub.execute_input":"2025-04-18T13:42:42.782944Z","iopub.status.idle":"2025-04-18T13:42:42.788465Z","shell.execute_reply.started":"2025-04-18T13:42:42.782919Z","shell.execute_reply":"2025-04-18T13:42:42.787801Z"}},"outputs":[],"execution_count":25},{"id":"ddb2603c","cell_type":"markdown","source":"# Functions","metadata":{}},{"id":"20b3418e","cell_type":"markdown","source":"## CNN Model Architecture","metadata":{}},{"id":"42edb036","cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, num_filters=32, size_filters=3, activation_func='relu', filter_org=1, num_dense=128, batch_normalisation=False, dropout_rate=0.2, input_channels=3, num_classes=10, num_conv=5):\n        '''\n        num_filters: Number of filters in each layer --> 32,64,etc\n        size_filters: Size of each filter (=F) --> 5,10,etc\n        activation_func: Activation function for the convolutional layers --> ReLU, GeLU,SiLU, Mish\n        filter_org: Ratio of number of filters in i+1th layer to number of filters in ith layer --> 1,0.5,2,etc\n        num_dense: Number of neurons in dense layer --> 128\n        batch_normalisation: Whether or not to apply batch normalisation after convolution layers --> True, False\n        dropout_rate: Fraction of neurons to randomly drop (=p) --> 0.2 to 0.5\n        input_channels: number of channels in input layer --> 3 (RGB)\n        num_classes: Number of Classes in the iNaturalist Dataset --> 10\n        num_conv: number of Conv-activation-maxpool blocks in the CNN model --> given:5\n        '''\n        super(CNN, self).__init__()\n        self.layers=nn.ModuleList()\n\n        def get_activation(name):\n            if name == 'relu':\n                return nn.ReLU()\n            elif name == 'gelu':\n                return nn.GELU()\n            elif name == 'silu':\n                return nn.SiLU()\n            elif name == 'mish':\n                return nn.Mish()\n\n        for layer in range(num_conv):\n            out_channels=int(num_filters*((filter_org)**(layer)))\n            self.layers.append(nn.Conv2d(in_channels=input_channels, out_channels=out_channels, kernel_size=size_filters, padding=size_filters//2))\n            if batch_normalisation==True:\n                self.layers.append(nn.BatchNorm2d(out_channels))\n            input_channels=out_channels\n            self.layers.append(get_activation(name=activation_func))    \n            self.layers.append(nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n        if batch_normalisation==True:\n            self.fc_layers = nn.Sequential(nn.Linear(input_channels, num_dense),get_activation(name=activation_func), nn.BatchNorm1d(num_dense), nn.Dropout(p=dropout_rate), nn.Linear(num_dense, num_classes))\n        elif batch_normalisation==False:\n            self.fc_layers = nn.Sequential(nn.Linear(input_channels, num_dense),get_activation(name=activation_func), nn.Dropout(p=dropout_rate), nn.Linear(num_dense, num_classes))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        x = self.adaptive_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_layers(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:36:14.756023Z","iopub.execute_input":"2025-04-18T12:36:14.756316Z","iopub.status.idle":"2025-04-18T12:36:14.766409Z","shell.execute_reply.started":"2025-04-18T12:36:14.756292Z","shell.execute_reply":"2025-04-18T12:36:14.765678Z"}},"outputs":[],"execution_count":3},{"id":"23a22440","cell_type":"markdown","source":"## Loading Data","metadata":{}},{"id":"e765a9ae","cell_type":"code","source":"def get_dataloaders(dir='/kaggle/input/nature-12k/inaturalist_12K/train',augment='No',split=0.2,batch_size=64):\n    labels = datasets.ImageFolder(root=dir).targets\n    \n    val_transforms = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225])\n    ])\n    if augment=='Yes':\n        train_transforms = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2,\n                                saturation=0.2, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225])\n        ])  \n    elif augment=='No':\n        train_transforms=val_transforms\n\n    splitter = StratifiedShuffleSplit(n_splits=1, test_size=split, random_state=42)\n    train_idx, val_idx = next(splitter.split(torch.zeros(len(labels)), labels))\n\n    train_dataset=datasets.ImageFolder(root=dir,transform=train_transforms)\n    val_dataset=datasets.ImageFolder(root=dir,transform=val_transforms)\n\n    train_dataset = Subset(train_dataset, train_idx)\n    val_dataset = Subset(val_dataset, val_idx)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader,val_loader\n\ndef test_dataloader(dir='/kaggle/input/nature-12k/inaturalist_12K/val',batch_size=32):    \n    all_transforms = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225])\n    ])\n    \n    test_dataset=datasets.ImageFolder(root=dir,transform=all_transforms)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n\n    return test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:36:21.106915Z","iopub.execute_input":"2025-04-18T12:36:21.107534Z","iopub.status.idle":"2025-04-18T12:36:21.115562Z","shell.execute_reply.started":"2025-04-18T12:36:21.107506Z","shell.execute_reply":"2025-04-18T12:36:21.114805Z"}},"outputs":[],"execution_count":4},{"id":"be2b4695","cell_type":"markdown","source":"## Validating correctness of data split","metadata":{}},{"id":"5b27c552","cell_type":"code","source":"def get_class_counts(dataloader):\n    dataset = dataloader.dataset\n    targets = [dataset.dataset.targets[i] for i in dataset.indices]\n    total=0\n    for cls,count in sorted(Counter(targets).items()):\n        print(f'Class{cls}: {count} samples')\n        total+=count\n    print(f'total samples={total}')\n    return total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:36:26.044142Z","iopub.execute_input":"2025-04-18T12:36:26.044761Z","iopub.status.idle":"2025-04-18T12:36:26.048907Z","shell.execute_reply.started":"2025-04-18T12:36:26.044724Z","shell.execute_reply":"2025-04-18T12:36:26.048199Z"}},"outputs":[],"execution_count":5},{"id":"e431716a","cell_type":"markdown","source":"## Optimizer","metadata":{}},{"id":"29a5c652","cell_type":"code","source":"def get_optimizer(optim,lr,model):\n    if optim=='sgd':\n        return (torch.optim.SGD(model.parameters(), lr, weight_decay=0, momentum=0))\n    elif optim=='momentum':\n        return (torch.optim.SGD(model.parameters(), lr, weight_decay=0, momentum=0.9))\n    elif optim=='adam':\n        return (torch.optim.Adam(model.parameters(), lr, weight_decay=0.005))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:36:29.479724Z","iopub.execute_input":"2025-04-18T12:36:29.480385Z","iopub.status.idle":"2025-04-18T12:36:29.485162Z","shell.execute_reply.started":"2025-04-18T12:36:29.480357Z","shell.execute_reply":"2025-04-18T12:36:29.484331Z"}},"outputs":[],"execution_count":6},{"id":"d1585915","cell_type":"markdown","source":"# Best Model","metadata":{}},{"id":"650238db","cell_type":"code","source":"# Parameters of the Best Model\n# Validation Accuracy = 40.4%\nactivation_func= 'relu'\nbatch_normalisation=True\nbatch_size=32\ndata_augmentation='No'\ndropout_rate=0.20426922413644705\nfilter_org=1\nlearning_rate=0.0032148335356218384\nnum_dense=256\nnum_filters=64\noptimizer='momentum'\nsize_filters=3\n\nmodel=CNN(num_filters, size_filters, activation_func, filter_org, num_dense, batch_normalisation, dropout_rate, input_channels=3, num_classes=10, num_conv=5)\nsummary(model,input_size=(3,224,224),device='cpu')\n\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n    model = nn.DataParallel(model)\n\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer=get_optimizer(optimizer,learning_rate,model=model)\ntrain_loader,val_loader=get_dataloaders('/kaggle/input/nature-12k/inaturalist_12K/train',data_augmentation,0.2,batch_size)\n\nprint('\\nNo. of samples in each class in training data:')\ntrain_count = get_class_counts(train_loader)\nprint('\\nNo. of samples in each class in Validation data:')\nval_count = get_class_counts(val_loader)\n\nprint('\\nPercentage of train data kept aside for validation={:.2f}%'.format(val_count*100/(train_count+val_count)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:41:02.297655Z","iopub.execute_input":"2025-04-18T12:41:02.298509Z","iopub.status.idle":"2025-04-18T12:41:20.736378Z","shell.execute_reply.started":"2025-04-18T12:41:02.298480Z","shell.execute_reply":"2025-04-18T12:41:20.735555Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n       BatchNorm2d-2         [-1, 64, 224, 224]             128\n              ReLU-3         [-1, 64, 224, 224]               0\n         MaxPool2d-4         [-1, 64, 112, 112]               0\n            Conv2d-5         [-1, 64, 112, 112]          36,928\n       BatchNorm2d-6         [-1, 64, 112, 112]             128\n              ReLU-7         [-1, 64, 112, 112]               0\n         MaxPool2d-8           [-1, 64, 56, 56]               0\n            Conv2d-9           [-1, 64, 56, 56]          36,928\n      BatchNorm2d-10           [-1, 64, 56, 56]             128\n             ReLU-11           [-1, 64, 56, 56]               0\n        MaxPool2d-12           [-1, 64, 28, 28]               0\n           Conv2d-13           [-1, 64, 28, 28]          36,928\n      BatchNorm2d-14           [-1, 64, 28, 28]             128\n             ReLU-15           [-1, 64, 28, 28]               0\n        MaxPool2d-16           [-1, 64, 14, 14]               0\n           Conv2d-17           [-1, 64, 14, 14]          36,928\n      BatchNorm2d-18           [-1, 64, 14, 14]             128\n             ReLU-19           [-1, 64, 14, 14]               0\n        MaxPool2d-20             [-1, 64, 7, 7]               0\nAdaptiveAvgPool2d-21             [-1, 64, 1, 1]               0\n           Linear-22                  [-1, 256]          16,640\n             ReLU-23                  [-1, 256]               0\n      BatchNorm1d-24                  [-1, 256]             512\n          Dropout-25                  [-1, 256]               0\n           Linear-26                   [-1, 10]           2,570\n================================================================\nTotal params: 169,866\nTrainable params: 169,866\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 106.07\nParams size (MB): 0.65\nEstimated Total Size (MB): 107.29\n----------------------------------------------------------------\nUsing 2 GPUs\n\nNo. of samples in each class in training data:\nClass0: 800 samples\nClass1: 800 samples\nClass2: 800 samples\nClass3: 800 samples\nClass4: 799 samples\nClass5: 800 samples\nClass6: 800 samples\nClass7: 800 samples\nClass8: 800 samples\nClass9: 800 samples\ntotal samples=7999\n\nNo. of samples in each class in Validation data:\nClass0: 200 samples\nClass1: 200 samples\nClass2: 200 samples\nClass3: 200 samples\nClass4: 200 samples\nClass5: 200 samples\nClass6: 200 samples\nClass7: 200 samples\nClass8: 200 samples\nClass9: 200 samples\ntotal samples=2000\n\nPercentage of train data kept aside for validation=20.00%\n","output_type":"stream"}],"execution_count":11},{"id":"f3d78fe2","cell_type":"code","source":"# Train\nnum_epochs=20\nprint(\"training...\")\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss=0\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss+=loss.item()\n    avg_loss=total_loss/len(train_loader)\n    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, avg_loss))\n\n    # Validation on training data\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print('Train Accuracy: {:.2f}%'.format(accuracy))\n\n    # Validation on validation data\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print('Validation Accuracy: {:.2f}%'.format(accuracy))\n\ntest_loader=test_dataloader(batch_size=32)\n# Test\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint('Test Accuracy: {:.2f}%'.format(accuracy))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:44:55.445815Z","iopub.execute_input":"2025-04-18T12:44:55.446149Z","iopub.status.idle":"2025-04-18T13:23:18.949006Z","shell.execute_reply.started":"2025-04-18T12:44:55.446122Z","shell.execute_reply":"2025-04-18T13:23:18.948061Z"}},"outputs":[{"name":"stdout","text":"training...\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20], Loss: 2.1974\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 24.05%\nValidation Accuracy: 23.25%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20], Loss: 2.1068\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 26.10%\nValidation Accuracy: 27.35%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20], Loss: 2.0623\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 28.32%\nValidation Accuracy: 27.45%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20], Loss: 2.0136\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 30.12%\nValidation Accuracy: 29.00%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20], Loss: 1.9854\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 31.58%\nValidation Accuracy: 30.90%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [6/20], Loss: 1.9635\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 30.57%\nValidation Accuracy: 28.80%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [7/20], Loss: 1.9274\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 32.57%\nValidation Accuracy: 32.00%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [8/20], Loss: 1.9089\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 35.53%\nValidation Accuracy: 33.75%\n","output_type":"stream"},{"name":"stderr","text":"                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch [9/20], Loss: 1.8993\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 31.35%\nValidation Accuracy: 28.90%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [10/20], Loss: 1.8642\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 37.54%\nValidation Accuracy: 34.55%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [11/20], Loss: 1.8557\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 34.05%\nValidation Accuracy: 33.40%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [12/20], Loss: 1.8317\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 35.20%\nValidation Accuracy: 33.70%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [13/20], Loss: 1.8185\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 36.49%\nValidation Accuracy: 34.85%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [14/20], Loss: 1.8070\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 39.63%\nValidation Accuracy: 38.05%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [15/20], Loss: 1.7972\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 36.89%\nValidation Accuracy: 34.70%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [16/20], Loss: 1.7719\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 39.43%\nValidation Accuracy: 36.20%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [17/20], Loss: 1.7653\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 38.90%\nValidation Accuracy: 35.05%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [18/20], Loss: 1.7439\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 42.04%\nValidation Accuracy: 37.30%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [19/20], Loss: 1.7400\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 43.37%\nValidation Accuracy: 40.00%\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch [20/20], Loss: 1.7178\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"Train Accuracy: 42.37%\nValidation Accuracy: 36.75%\nTest Accuracy: 36.40%\n","output_type":"stream"}],"execution_count":13},{"id":"761dd245","cell_type":"markdown","source":"## Grid","metadata":{}},{"id":"86acd323","cell_type":"code","source":"def get_images(dir=\"/kaggle/input/nature-12k/inaturalist_12K/val\"):\n    class_images = {}\n    all_transforms = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225]),\n    ])\n    \n    for class_name in os.listdir(dir):\n        class_path = os.path.join(dir, class_name)\n        images = [f for f in os.listdir(class_path)]\n        img_path = os.path.join(class_path, random.choice(images))\n        img = Image.open(img_path).convert(\"RGB\")\n        img_tensor = all_transforms(img)\n        class_images[class_name] = img_tensor\n    return class_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:45:17.246260Z","iopub.execute_input":"2025-04-18T13:45:17.246909Z","iopub.status.idle":"2025-04-18T13:45:17.251890Z","shell.execute_reply.started":"2025-04-18T13:45:17.246882Z","shell.execute_reply":"2025-04-18T13:45:17.251306Z"}},"outputs":[],"execution_count":30},{"id":"11300e87-30d2-486a-875f-49a1cfdf0382","cell_type":"code","source":"wandb.login(key='70a00ae1607c730fb9cd50b1268b191bec7a2901')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:29:38.366744Z","iopub.execute_input":"2025-04-18T13:29:38.367475Z","iopub.status.idle":"2025-04-18T13:29:38.436787Z","shell.execute_reply.started":"2025-04-18T13:29:38.367448Z","shell.execute_reply":"2025-04-18T13:29:38.436249Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mishita49\u001b[0m (\u001b[33mishita49-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":19},{"id":"ec3bcff6","cell_type":"code","source":"wandb.init(project=\"DA6401_Assign2\")\ntable = wandb.Table(columns=[\"Test Image\", \"True Label\", \"Predicted Label\"])\nclass_images = get_images()\nmodel.eval()\nlabels = datasets.ImageFolder('/kaggle/input/nature-12k/inaturalist_12K/val').classes\n\nwith torch.no_grad():\n    for class_name, img_tensor in class_images.items():\n        image = img_tensor.unsqueeze(0).to(device)\n        outputs = model(image)\n        _, predicted = torch.max(outputs.data, 1)\n        predicted_class = labels[predicted.item()]\n\n        img_disp = img_tensor.clone().detach().cpu()\n        img_disp = img_disp * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n        img_disp = img_disp + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n        img_disp = img_disp.clamp(0, 1)\n        table.add_data(wandb.Image(img_disp),class_name, predicted_class)\n\nwandb.log({\"10x3_grid\": table})\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:46:35.282752Z","iopub.execute_input":"2025-04-18T13:46:35.283124Z","iopub.status.idle":"2025-04-18T13:46:43.455252Z","shell.execute_reply.started":"2025-04-18T13:46:35.283102Z","shell.execute_reply":"2025-04-18T13:46:43.454459Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_134635-ybvcz4dh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2/runs/ybvcz4dh' target=\"_blank\">snowy-totem-75</a></strong> to <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2/runs/ybvcz4dh' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2/runs/ybvcz4dh</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">snowy-totem-75</strong> at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2/runs/ybvcz4dh' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2/runs/ybvcz4dh</a><br> View project at: <a href='https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2' target=\"_blank\">https://wandb.ai/ishita49-indian-institute-of-technology-madras/DA6401_Assign2</a><br>Synced 5 W&B file(s), 1 media file(s), 12 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250418_134635-ybvcz4dh/logs</code>"},"metadata":{}}],"execution_count":32}]}