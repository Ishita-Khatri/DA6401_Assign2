{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e360e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1be854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters=32, size_filters=3, activation_func='relu', filter_org=1, num_dense=128, input_channels=3, num_classes=10, num_conv=5):\n",
    "        '''\n",
    "        num_filters: Number of filters in each layer --> 32,64,etc\n",
    "        size_filters: Size of each filter (=F) --> 5,10,etc\n",
    "        activation_func: Activation function for the convolutional layers --> ReLU, GeLU,SiLU, Mish\n",
    "        filter_org: Ratio of number of filters in i+1th layer to number of filters in ith layer --> 1,0.5,2,etc\n",
    "        num_dense: Number of neurons in dense layer --> 128\n",
    "        input_channels: number of channels in input layer --> 3 (RGB)\n",
    "        num_classes: Number of Classes in the iNaturalist Dataset --> 10\n",
    "        num_conv: number of Conv-activation-maxpool blocks in the CNN model --> given:5\n",
    "        '''\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers=nn.ModuleList()\n",
    "\n",
    "        if activation_func == 'relu':\n",
    "            activation_layer = nn.ReLU()\n",
    "        elif activation_func == 'gelu':\n",
    "            activation_layer = nn.GELU()\n",
    "        elif activation_func == 'silu':\n",
    "            activation_layer = nn.SiLU()\n",
    "        elif activation_func == 'mish':\n",
    "            activation_layer = nn.Mish()\n",
    "\n",
    "        for layer in range(num_conv):\n",
    "            out_channels=int(num_filters*((filter_org)**(layer)))\n",
    "            conv_layer = nn.Conv2d(in_channels=input_channels, out_channels=out_channels, kernel_size=size_filters, padding=size_filters//2)\n",
    "            self.layers.append(conv_layer)\n",
    "            input_channels=out_channels\n",
    "            self.layers.append(activation_layer)                \n",
    "            self.layers.append(nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layers = nn.Sequential(nn.Linear(input_channels, num_dense),activation_layer, nn.Linear(num_dense, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dir='/kaggle/input/inaturalist-10-class/train',split=0.2,batch_size=64):\n",
    "    all_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = datasets.ImageFolder(root=dir, transform=all_transforms)\n",
    "    labels = dataset.targets\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=split, random_state=42)\n",
    "    train_idx, val_idx = next(splitter.split(torch.zeros(len(labels)), labels))\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optim,lr,model):\n",
    "    if optim=='sgd':\n",
    "        return (torch.optim.SGD(model.parameters(), lr, weight_decay=0, momentum=0))\n",
    "    elif optim=='momentum':\n",
    "        return (torch.optim.SGD(model.parameters(), lr, weight_decay=0, momentum=0.9))\n",
    "    elif optim=='adam':\n",
    "        return (torch.optim.Adam(model.parameters(), lr, weight_decay=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e186948",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = CNN(\n",
    "    num_filters=32,\n",
    "    size_filters=3,\n",
    "    activation_func='relu',\n",
    "    filter_org=2,\n",
    "    num_dense=128\n",
    ").to(device)\n",
    "\n",
    "summary(my_model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceead109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(\n",
    "#     project=\"DA6401_Assign2\",\n",
    "# )\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'num_filters':{\n",
    "            'values':[32,64]\n",
    "        },\n",
    "        'size_filters':{\n",
    "            'values':[3,5,10]\n",
    "        },\n",
    "        'activation_func':{\n",
    "            'values':['relu','gelu','silu','mish']\n",
    "        },\n",
    "        'filter_org':{\n",
    "            'values':[1,0.5,2]\n",
    "        },\n",
    "        'num_dense':{\n",
    "            'values':[128,256,512]\n",
    "        },\n",
    "        'batch_size':{\n",
    "            'values':[16,32,64]\n",
    "        },\n",
    "        'optimizer':{\n",
    "            'values':['sgd','momentum','adam']\n",
    "        },\n",
    "        'learning_rate':{\n",
    "            'min':0.0001,\n",
    "            'max':0.01\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assign2\", entity=\"ishita49-indian-institute-of-technology-madras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "        'num_filters':32,\n",
    "        'size_filters':3,\n",
    "        'activation_func':'relu',\n",
    "        'filter_org':2,\n",
    "        'num_dense':128,\n",
    "        'batch_size':64,\n",
    "        'optimizer':'sgd',\n",
    "        'learning_rate':0.005\n",
    "    }\n",
    "    \n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "\n",
    "    model = CNN(num_filters=config.num_filters,\n",
    "                size_filters=config.size_filters,\n",
    "                activation_func=config.activation_func,\n",
    "                filter_org=config.filter_org,\n",
    "                num_dense=config.num_dense,\n",
    "                num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer=get_optimizer(optim=config.optimizer,lr=config.learning_rate,model=model)\n",
    "    train_loader,val_loader=get_dataloaders(batch_size=config.batch_size)\n",
    "    # Train\n",
    "    num_epochs=20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss+=loss.item()\n",
    "        avg_loss=total_loss/len(train_loader)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, avg_loss))\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_loss})\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Validation Accuracy: {:.2f}%'.format(accuracy))\n",
    "\n",
    "    wandb.log({\"val_accuracy\": accuracy})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
